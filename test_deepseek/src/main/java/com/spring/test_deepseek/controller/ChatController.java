package com.spring.test_deepseek.controller;

import com.spring.test_deepseek.service.ChatService;
import org.springframework.ai.chat.messages.UserMessage;
import org.springframework.ai.chat.model.ChatResponse;
import org.springframework.ai.chat.prompt.Prompt;
import org.springframework.ai.ollama.OllamaChatModel;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;
import reactor.core.publisher.Flux;

import java.util.Map;

@RestController
@RequestMapping("/api/v1/test-model")
public class ChatController {


    private final OllamaChatModel chatModel;    //locally hosted AI model
    private final ChatService chatService;

    @Autowired
    public ChatController(OllamaChatModel chatModel, ChatService chatService) {
        this.chatModel = chatModel;
        this.chatService = chatService;
    }


    @GetMapping("/ai/generate")
    public Map<String, String> generate(@RequestParam(value = "message", defaultValue = "This is mock response") String message) {
        return Map.of("result", this.chatModel.call(message));
    }

    @GetMapping("/ai/generateStream")
    public Flux<ChatResponse> generateStream(@RequestParam(value = "message", defaultValue = "This is mock response") String message) {
        Prompt prompt = new Prompt(new UserMessage(message));
        return this.chatModel.stream(prompt);
    }

    @GetMapping("/prompt")
    public String getResponseFromDeepSeekAI(@RequestParam(value = "question") String question){
        return chatService.getResponseFromDeepSeekAI(question);
    }

    /**
     * Instead of waiting for the entire response, API streams tokens as they are generated by the LLM.
     * real-time streaming of responses as chunks of data were arrived
     * this is useful for chat applications where responses are progressively displayed instead of waiting for a full response
     * */
    @GetMapping("/prompt-as-stream")
    public Flux<String> getResponseFromDeepSeekAIWithStream(@RequestParam(value = "question") String question){
        return chatService.getResponseFromDeepSeekAIWithStream(question);
    }
}

